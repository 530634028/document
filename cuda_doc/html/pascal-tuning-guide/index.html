<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-us" xml:lang="en-us">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>
      <meta name="copyright" content="(C) Copyright 2005"></meta>
      <meta name="DC.rights.owner" content="(C) Copyright 2005"></meta>
      <meta name="DC.Type" content="concept"></meta>
      <meta name="DC.Title" content="Tuning CUDA Applications for Pascal"></meta>
      <meta name="abstract" content="The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Pascal Architecture."></meta>
      <meta name="description" content="The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Pascal Architecture."></meta>
      <meta name="DC.Coverage" content="Programming Guides"></meta>
      <meta name="DC.subject" content="CUDA Pascal, CUDA Pascal tuning, CUDA Pascal best practices, CUDA Pascal performance"></meta>
      <meta name="keywords" content="CUDA Pascal, CUDA Pascal tuning, CUDA Pascal best practices, CUDA Pascal performance"></meta>
      <meta name="DC.Format" content="XHTML"></meta>
      <meta name="DC.Identifier" content="abstract"></meta>
      <link rel="stylesheet" type="text/css" href="../common/formatting/commonltr.css"></link>
      <link rel="stylesheet" type="text/css" href="../common/formatting/site.css"></link>
      <title>Pascal Tuning Guide :: CUDA Toolkit Documentation</title>
      <!--[if lt IE 9]>
      <script src="../common/formatting/html5shiv-printshiv.min.js"></script>
      <![endif]-->
      <script type="text/javascript" charset="utf-8" src="../common/scripts/tynt/tynt.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.min.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.ba-hashchange.min.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.scrollintoview.min.js"></script>
      <script type="text/javascript" src="../search/htmlFileList.js"></script>
      <script type="text/javascript" src="../search/htmlFileInfoList.js"></script>
      <script type="text/javascript" src="../search/nwSearchFnt.min.js"></script>
      <script type="text/javascript" src="../search/stemmers/en_stemmer.min.js"></script>
      <script type="text/javascript" src="../search/index-1.js"></script>
      <script type="text/javascript" src="../search/index-2.js"></script>
      <script type="text/javascript" src="../search/index-3.js"></script>
      <link rel="canonical" href="http://docs.nvidia.com/cuda/pascal-tuning-guide/index.html"></link>
      <link rel="stylesheet" type="text/css" href="../common/formatting/qwcode.highlight.css"></link>
   </head>
   <body>
      
      <header id="header"><span id="company">NVIDIA</span><span id="site-title">CUDA Toolkit Documentation</span><form id="search" method="get" action="search">
            <input type="text" name="search-text"></input><fieldset id="search-location">
               <legend>Search In:</legend>
               <label><input type="radio" name="search-type" value="site"></input>Entire Site</label>
               <label><input type="radio" name="search-type" value="document"></input>Just This Document</label></fieldset>
            <button type="reset">clear search</button>
            <button id="submit" type="submit">search</button></form>
      </header>
      <div id="site-content">
         <nav id="site-nav">
            <div class="category closed"><a href="../index.html" title="The root of the site.">CUDA Toolkit
                  v8.0</a></div>
            <div class="category"><a href="index.html" title="Pascal Tuning Guide">Pascal Tuning Guide</a></div>
            <ul>
               <li>
                  <div class="section-link"><a href="#tuning-cuda-applications-for-pascal">1.&nbsp;Pascal Tuning Guide</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#nvidia-pascal-compute-architecture">1.1.&nbsp;NVIDIA Pascal Compute Architecture</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#cuda-best-practices">1.2.&nbsp;CUDA Best Practices</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#application-compatibility">1.3.&nbsp;Application Compatibility</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#pascal-tuning">1.4.&nbsp;Pascal Tuning</a></div>
                        <ul>
                           <li>
                              <div class="section-link"><a href="#sm">1.4.1.&nbsp;Streaming Multiprocessor</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#sm-scheduling">1.4.1.1.&nbsp;Instruction Scheduling</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#sm-occupancy">1.4.1.2.&nbsp;Occupancy</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#arithmetic-primitives">1.4.2.&nbsp;New Arithmetic Primitives</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#fp16">1.4.2.1.&nbsp;FP16 Arithmetic Support</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#int8">1.4.2.2.&nbsp;INT8 Dot Product</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#memory-throughput">1.4.3.&nbsp;Memory Throughput</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#hbm2">1.4.3.1.&nbsp;High Bandwidth Memory 2 DRAM</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#l1-cache">1.4.3.2.&nbsp;Unified L1/Texture Cache</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#atomic-ops">1.4.4.&nbsp;Atomic Memory Operations</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#shared-memory">1.4.5.&nbsp;Shared Memory</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#shared-memory-capacity">1.4.5.1.&nbsp;Shared Memory Capacity</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#shared-memory-bandwidth">1.4.5.2.&nbsp;Shared Memory Bandwidth</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#inter-gpu-communication">1.4.6.&nbsp;Inter-GPU Communication</a></div>
                              <ul>
                                 <li>
                                    <div class="section-link"><a href="#nvlink">1.4.6.1.&nbsp;NVLink Interconnect</a></div>
                                 </li>
                                 <li>
                                    <div class="section-link"><a href="#gpudirect">1.4.6.2.&nbsp;GPUDirect RDMA Bandwidth</a></div>
                                 </li>
                              </ul>
                           </li>
                           <li>
                              <div class="section-link"><a href="#preemption">1.4.7.&nbsp;Compute Preemption</a></div>
                           </li>
                           <li>
                              <div class="section-link"><a href="#unified-memory">1.4.8.&nbsp;Unified Memory Improvements</a></div>
                           </li>
                        </ul>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#revision-history">A.&nbsp;Revision History</a></div>
               </li>
            </ul>
         </nav>
         <div id="resize-nav"></div>
         <nav id="search-results">
            <h2>Search Results</h2>
            <ol></ol>
         </nav>
         
         <div id="contents-container">
            <div id="breadcrumbs-container">
               <div id="release-info">Pascal Tuning Guide
                  (<a href="../../pdf/Pascal_Tuning_Guide.pdf">PDF</a>)
                  -
                  
                  v8.0
                  (<a href="https://developer.nvidia.com/cuda-toolkit-archive">older</a>)
                  -
                  Last updated January 11, 2017
                  -
                  <a href="mailto:cudatools@nvidia.com?subject=CUDA Toolkit Documentation Feedback: Pascal Tuning Guide">Send Feedback</a>
                  -
                  <span class="st_facebook"></span><span class="st_twitter"></span><span class="st_linkedin"></span><span class="st_reddit"></span><span class="st_slashdot"></span><span class="st_tumblr"></span><span class="st_sharethis"></span></div>
            </div>
            <article id="contents">
               <div class="topic nested0" id="abstract"><a name="abstract" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#abstract" name="abstract" shape="rect">Tuning CUDA Applications for Pascal</a></h2>
                  <div class="body conbody"></div>
               </div>
               <div class="topic concept nested0" xml:lang="en-US" id="tuning-cuda-applications-for-pascal"><a name="tuning-cuda-applications-for-pascal" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#tuning-cuda-applications-for-pascal" name="tuning-cuda-applications-for-pascal" shape="rect">1.&nbsp;Pascal Tuning Guide</a></h2>
                  <div class="topic concept nested1" xml:lang="en-US" id="nvidia-pascal-compute-architecture"><a name="nvidia-pascal-compute-architecture" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#nvidia-pascal-compute-architecture" name="nvidia-pascal-compute-architecture" shape="rect">1.1.&nbsp;NVIDIA Pascal Compute Architecture</a></h3>
                     <div class="body conbody">
                        <p class="p">Pascal is NVIDIA's latest architecture for CUDA compute applications.
                           Pascal retains and extends the same CUDA programming model provided by
                           previous NVIDIA architectures such as Kepler and Maxwell, and applications
                           that follow the best practices for those architectures should typically
                           see speedups on the Pascal architecture without any code changes. This
                           guide summarizes the ways that an application can be fine-tuned to gain
                           additional speedups by leveraging Pascal architectural features.<a name="fnsrc_1" href="#fntarg_1" shape="rect"><sup>1</sup></a></p>
                        <p class="p">Pascal architecture comprises two major variants: GP100 and GP104.<a name="fnsrc_2" href="#fntarg_2" shape="rect"><sup>2</sup></a>
                           A detailed overview of the major improvements in GP100 and GP104 over
                           earlier NVIDIA architectures are described in a pair of white papers
                           entitled <a class="xref" href="http://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf" target="_blank" shape="rect">NVIDIA Tesla P100: The Most Advanced Datacenter
                              Accelerator Ever Built</a> for GP100 and <a class="xref" href="http://international.download.nvidia.com/geforce-com/international/pdfs/GeForce_GTX_1080_Whitepaper_FINAL.pdf" target="_blank" shape="rect">NVIDIA GeForce GTX 1080: Gaming Perfected</a>
                           for GP104.
                        </p>
                        <p class="p">For further details on the programming features discussed in this
                           guide, please refer to the <a class="xref" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/" target="_blank" shape="rect">CUDA C Programming Guide</a>. Some of
                           the Pascal features described in this guide are specific to either
                           GP100 or GP104, as noted; if not specified, features apply to both
                           Pascal variants.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="cuda-best-practices"><a name="cuda-best-practices" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#cuda-best-practices" name="cuda-best-practices" shape="rect">1.2.&nbsp;CUDA Best Practices</a></h3>
                     <div class="body conbody">
                        <p class="p">The performance guidelines and best practices described in the <a class="xref" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/" target="_blank" shape="rect">CUDA C Programming Guide</a> and the
                           <a class="xref" href="http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/" target="_blank" shape="rect">CUDA C Best Practices Guide</a> apply
                           to all CUDA-capable GPU architectures. Programmers must primarily focus
                           on following those recommendations to achieve the best performance.
                        </p>
                        <div class="p">The high-priority recommendations from those guides are as follows:
                           
                           <ul class="ul">
                              <li class="li">Find ways to parallelize sequential code,</li>
                              <li class="li">Minimize data transfers between the host and the device,</li>
                              <li class="li">Adjust kernel launch configuration to maximize device
                                 utilization,
                              </li>
                              <li class="li">Ensure global memory accesses are coalesced,</li>
                              <li class="li">Minimize redundant accesses to global memory whenever
                                 possible,
                              </li>
                              <li class="li">Avoid long sequences of diverged execution by threads within
                                 the same warp.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="application-compatibility"><a name="application-compatibility" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#application-compatibility" name="application-compatibility" shape="rect">1.3.&nbsp;Application Compatibility</a></h3>
                     <div class="body conbody">
                        <p class="p">Before addressing specific performance tuning issues
                           covered in this guide, refer to the <a class="xref" href="http://docs.nvidia.com/cuda/pascal-compatibility-guide/" target="_blank" shape="rect">Pascal Compatibility Guide for CUDA
                              Applications</a> to ensure that your application is compiled in a
                           way that is compatible with Pascal.
                        </p>
                     </div>
                  </div>
                  <div class="topic concept nested1" xml:lang="en-US" id="pascal-tuning"><a name="pascal-tuning" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#pascal-tuning" name="pascal-tuning" shape="rect">1.4.&nbsp;Pascal Tuning</a></h3>
                     <div class="topic concept nested2" xml:lang="en-US" id="sm"><a name="sm" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#sm" name="sm" shape="rect">1.4.1.&nbsp;Streaming Multiprocessor</a></h3>
                        <div class="body conbody">
                           <p class="p">The Pascal Streaming Multiprocessor (SM) is in many respects
                              similar to that of Maxwell. Pascal further improves the already
                              excellent power efficiency provided by the Maxwell architecture
                              through both an improved 16-nm FinFET manufacturing process and
                              various architectural modifications.
                           </p>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="sm-scheduling"><a name="sm-scheduling" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#sm-scheduling" name="sm-scheduling" shape="rect">1.4.1.1.&nbsp;Instruction Scheduling</a></h3>
                           <div class="body conbody">
                              <p class="p">Like Maxwell, Pascal employs a power-of-two number of CUDA
                                 Cores per partition. This simplifies scheduling compared to
                                 Kepler, since each of the SM's warp schedulers issue to a
                                 dedicated set of CUDA Cores equal to the warp width (32). Each
                                 warp scheduler still has the flexibility to dual-issue (such as
                                 issuing a math operation to a CUDA Core in the same cycle as a
                                 memory operation to a load/store unit), but single-issue is now
                                 sufficient to fully utilize all CUDA Cores.
                              </p>
                              <p class="p">GP100 and GP104 designs incorporate different numbers of
                                 CUDA Cores per SM. Like Maxwell, each GP104 SM provides four
                                 warp schedulers managing a total of 128 single-precision (FP32)
                                 and four double-precision (FP64) cores. A GP104 processor
                                 provides up to 20 SMs, and the similar GP102 design provides up
                                 to 30 SMs.
                              </p>
                              <p class="p">By contrast GP100 provides smaller but more numerous SMs.
                                 Each GP100 provides up to 60 SMs.<a name="fnsrc_3" href="#fntarg_3" shape="rect"><sup>3</sup></a> Each SM contains two warp schedulers managing a
                                 total of 64 FP32 and 32 FP64 cores. The resulting 2:1 ratio of
                                 FP32 to FP64 cores aligns well with GP100's new datapath
                                 configuration, allowing Pascal to process FP64 workloads more
                                 efficiently than Kepler GK210, the previous NVIDIA architecture
                                 to emphasize FP64 performance.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="sm-occupancy"><a name="sm-occupancy" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#sm-occupancy" name="sm-occupancy" shape="rect">1.4.1.2.&nbsp;Occupancy</a></h3>
                           <div class="body conbody">
                              <div class="p">The maximum number of concurrent warps per SM remains the
                                 same as in Maxwell and Kepler (i.e., 64), and other <a class="xref" href="http://developer.download.nvidia.com/compute/cuda/CUDA_Occupancy_calculator.xls" target="_blank" shape="rect">factors influencing warp
                                    occupancy</a> remain similar as well:
                                 
                                 <ul class="ul">
                                    <li class="li">The register file size (64k 32-bit registers) is the
                                       same as that of Maxwell and Kepler GK110.
                                    </li>
                                    <li class="li">The maximum registers per thread, 255, matches that of
                                       Kepler GK110 and Maxwell. As with previous architectures,
                                       experimentation should be used to determine the optimum
                                       balance of register spilling vs. occupancy, however.
                                    </li>
                                    <li class="li">The maximum number of thread blocks per SM is 32, the
                                       same as Maxwell and an increase of 2x over Kepler. Compared
                                       to Kepler, Pascal should see an automatic occupancy
                                       improvement for kernels with thread blocks of 64 or fewer
                                       threads (shared memory and register file resource
                                       requirements permitting).
                                    </li>
                                    <li class="li">Shared memory capacity per SM is 64KB for GP100 and 96KB
                                       for GP104. For comparison, Maxwell and Kepler GK210 provided
                                       96KB and up to 112KB of shared memory, respectively. But each
                                       GP100 SM contains fewer CUDA Cores, so the shared
                                       memory available per core actually increases on
                                       GP100. The maximum shared memory per block remains limited at
                                       48KB as with prior architectures
                                       (see <a class="xref" href="index.html#shared-memory-capacity" shape="rect">Shared Memory Capacity</a>).
                                    </li>
                                 </ul>
                              </div>
                              <p class="p">As such, developers can expect similar occupancy as on Maxwell
                                 without changes to their application. As a result of scheduling
                                 improvements relative to Kepler, warp occupancy requirements
                                 (i.e., available parallelism) needed for maximum device
                                 utilization are generally reduced.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="arithmetic-primitives"><a name="arithmetic-primitives" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#arithmetic-primitives" name="arithmetic-primitives" shape="rect">1.4.2.&nbsp;New Arithmetic Primitives</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="fp16"><a name="fp16" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#fp16" name="fp16" shape="rect">1.4.2.1.&nbsp;FP16 Arithmetic Support</a></h3>
                           <div class="body conbody">
                              <p class="p">Pascal provides improved FP16 support for applications, like
                                 deep learning, that are tolerant of low floating-point
                                 precision. The <samp class="ph codeph">half</samp> type is used to represent
                                 FP16 values on the device. As with Kepler and Maxwell, FP16 storage can be
                                 used to reduce the required memory footprint and bandwidth
                                 compared to FP32 or FP64 storage. Pascal also adds support for
                                 native FP16 instructions. Peak FP16 throughput is attained by
                                 using a paired operation to perform two FP16 instructions per
                                 core simultaneously. To be eligible for the paired operation
                                 the operands must be stored in a <samp class="ph codeph">half2</samp> vector
                                 type. GP100 and GP104 provide different FP16 throughputs.
                                 GP100, designed with training deep neural networks in mind,
                                 provides FP16 throughput up to 2x that of FP32 arithmetic. On
                                 GP104, FP16 throughput is lower, 1/64th that of FP32. However,
                                 compensating for reduced FP16 throughput, GP104 provides
                                 additional high-throughput INT8 support not available in
                                 GP100.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="int8"><a name="int8" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#int8" name="int8" shape="rect">1.4.2.2.&nbsp;INT8 Dot Product</a></h3>
                           <div class="body conbody">
                              <p class="p">GP104 provides specialized instructions for two-way and
                                 four-way integer dot products. These are well suited for
                                 accelerating Deep Learning inference workloads. The
                                 <samp class="ph codeph">__dp4a</samp> intrinsic computes a dot product of
                                 four 8-bit integers with accumulation into a 32-bit integer.
                                 Similarly, <samp class="ph codeph">__dp2a</samp> performs a two-element dot
                                 product between two 16-bit integers in one vector, and two
                                 8-bit integers in another with accumulation into a 32-bit
                                 integer. Both instructions offer a throughput equal to that of
                                 FP32 arithmetic.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="memory-throughput"><a name="memory-throughput" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#memory-throughput" name="memory-throughput" shape="rect">1.4.3.&nbsp;Memory Throughput</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="hbm2"><a name="hbm2" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#hbm2" name="hbm2" shape="rect">1.4.3.1.&nbsp;High Bandwidth Memory 2 DRAM</a></h3>
                           <div class="body conbody">
                              <p class="p">GP100 uses High Bandwidth Memory 2 (HBM2) for its DRAM. HBM2
                                 memories are stacked on a single silicon package along with the
                                 GPU die. This allows much wider interfaces at similar power
                                 compared to traditional GDDR technology. GP100 is linked to up
                                 to four stacks of HBM2 and uses two 512-bit memory controllers
                                 for each stack. The effective width of the memory bus is then
                                 4096 bits, a significant increase over the 384 bits in GM200.
                                 This allows a tremendous boost in peak bandwidth even at
                                 reduced memory clocks. Thus, the GP100 equipped Tesla P100 has
                                 a peak bandwidth of 732 GB/s with a modest 715 MHz memory
                                 clock. DRAM access latencies remain similar to those observed
                                 on Maxwell.
                              </p>
                              <p class="p">In order to hide DRAM latencies at full HBM2 bandwidth, more
                                 memory accesses must be kept in flight compared to GPUs
                                 equipped with traditional GDDR5. Helpfully, the large
                                 complement of SMs in GP100 will typically boost the number of
                                 concurrent threads (and thus reads-in-flight) compared to
                                 previous architectures. Resource constrained kernels that are
                                 limited to low occupancy may benefit from increasing the number
                                 of concurrent memory accesses per thread.
                              </p>
                              <p class="p">Like Kepler GK210, the GP100 GPU's register files, shared
                                 memories, L1 and L2 caches, and DRAM are all protected by
                                 Single-Error Correct Double-Error Detect (SECDED) ECC code.
                                 When enabling ECC support on a Kepler GK210, the available DRAM
                                 would be reduced by 6.25% to allow for the storage of ECC bits.
                                 Fetching ECC bits for each memory transaction also reduced the
                                 effective bandwidth by approximately 20% compared to the same
                                 GPU with ECC disabled. HBM2 memories, on the other hand,
                                 provide dedicated ECC resources, allowing overhead-free ECC
                                 protection.<a name="fnsrc_4" href="#fntarg_4" shape="rect"><sup>4</sup></a></p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="l1-cache"><a name="l1-cache" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#l1-cache" name="l1-cache" shape="rect">1.4.3.2.&nbsp;Unified L1/Texture Cache</a></h3>
                           <div class="body conbody">
                              <p class="p">Like Maxwell, Pascal combines the functionality of the L1
                                 and texture caches into a unified L1/Texture cache which acts
                                 as a coalescing buffer for memory accesses, gathering up the
                                 data requested by the threads of a warp prior to delivery of
                                 that data to the warp.  This function previously was served by
                                 the separate L1 cache in Fermi and Kepler.
                              </p>
                              <p class="p">By default, GP100 caches global loads in the L1/Texture
                                 cache. In contrast, GP104 follows Kepler and Maxwell in caching
                                 global loads in L2 only, unless using the <dfn class="term">LDG</dfn>
                                 read-only data cache mechanism introduced in Kepler. As with
                                 previous  architectures, GP104 allows the developer to opt-in
                                 to caching all global loads in the unified L1/Texture cache by
                                 passing the <samp class="ph codeph">-Xptxas -dlcm=ca</samp> flag to
                                 <samp class="ph codeph">nvcc</samp> at compile time. 
                              </p>
                              <p class="p">Kepler serviced loads at a granularity of 128B when L1 caching
                                 of global loads was enabled and 32B otherwise. On Pascal the data
                                 access unit is 32B regardless of whether global loads are cached in
                                 L1. So it is no longer necessary to turn off L1 caching in order to
                                 reduce wasted global memory transactions associated with
                                 uncoalesced accesses.
                              </p>
                              <p class="p">Unlike Maxwell but similar to Kepler, Pascal caches
                                 thread-local memory in the L1 cache. This can mitigate the cost 
                                 of register spills compared to Maxwell. The balance of occupancy
                                 versus spilling should therefore be re-evaluated to ensure best
                                 performance.
                              </p>
                              <p class="p">Two new device attributes were added in CUDA Toolkit 6.0:
                                 <samp class="ph codeph">globalL1CacheSupported</samp> and
                                 <samp class="ph codeph">localL1CacheSupported</samp>.  Developers who wish to
                                 have separately-tuned paths for various architecture
                                 generations can use these fields to simplify the path selection
                                 process.
                              </p>
                              <div class="note note"><span class="notetitle">Note:</span> Enabling caching of globals in GP104 can affect
                                 occupancy. If per-thread-block SM resource usage would result
                                 in zero occupancy with caching enabled, the CUDA driver will
                                 override the caching selection to allow the kernel launch to
                                 succeed. This situation is reported by the profiler.
                              </div>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="atomic-ops"><a name="atomic-ops" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#atomic-ops" name="atomic-ops" shape="rect">1.4.4.&nbsp;Atomic Memory Operations</a></h3>
                        <div class="body conbody">
                           <p class="p">Like Maxwell, Pascal provides native <em class="ph i">shared</em> memory atomic
                              operations for 32-bit integer arithmetic, along with native 32 or
                              64-bit compare-and-swap (CAS). Developers coming from Kepler, where
                              shared memory atomics were implemented in software using a
                              lock/update/unlock sequence, should see a large performance
                              improvement particularly for heavily contended shared-memory
                              atomics.
                           </p>
                           <p class="p">Pascal also extends atomic addition in global memory to function
                              on FP64 data. The <samp class="ph codeph">atomicAdd()</samp> function in CUDA has
                              thus been generalized to support 32 and 64-bit integer and
                              floating-point types. The rounding mode for all floating-point
                              atomic operations is round-to-nearest-even in Pascal (in Kepler,
                              FP32 atomic addition used round-to-zero). As in previous
                              generations FP32 <samp class="ph codeph">atomicAdd()</samp> flushes denormalized
                              values to zero.
                           </p>
                           <p class="p">For GP100 atomic operations may target the
                              memories of peer GPUs connected through NVLink. Peer-to-peer
                              atomics over NVLink use the same API as atomics targeting global
                              memory. GPUs connected via PCIE do not support this feature.
                           </p>
                           <p class="p">Pascal GPUs provide support system-wide atomic operations
                              targeting <dfn class="term">migratable allocations</dfn><a name="fnsrc_5" href="#fntarg_5" shape="rect"><sup>5</sup></a> If system-wide atomic visibility is
                              desired, operations targeting migratable memory must specify a
                              system scope by using the <samp class="ph codeph">atomic[Op]_system()</samp>
                              intrinsics<a name="fnsrc_6" href="#fntarg_6" shape="rect"><sup>6</sup></a>. Using the device-scope atomics
                              (e.g. <samp class="ph codeph">atomicAdd()</samp>) on migratable memory remains
                              valid, but enforces atomic visibility only within the local
                              GPU.
                           </p>
                           <div class="note note"><span class="notetitle">Note:</span> Given the potential for incorrect usage of atomic scopes, it
                              is recommended that applications use a tool like CUDA memcheck to
                              detect and eliminate errors.
                           </div>
                           <p class="p">As implemented for Pascal, system-wide atomics are intended to
                              allow developers to experiment with enhanced memory models. They
                              are implemented in software and some care is required to achieve
                              good performance. When an atomic targets a migratable address
                              backed by a remote memory space, the local processor page-faults so
                              that the kernel can migrate the appropriate memory page to local
                              memory. Then the usual hardware instructions are used to execute
                              the atomic. Since the page is now locally resident, subsequent
                              atomics from the same processor will not result in additional
                              page-faults. However, atomic updates from different processors can
                              incur frequent page-faults.
                           </p>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="shared-memory"><a name="shared-memory" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory" name="shared-memory" shape="rect">1.4.5.&nbsp;Shared Memory</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="shared-memory-capacity"><a name="shared-memory-capacity" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory-capacity" name="shared-memory-capacity" shape="rect">1.4.5.1.&nbsp;Shared Memory Capacity</a></h3>
                           <div class="body conbody">
                              <p class="p">For Kepler, shared memory and the L1 cache shared
                                 the same on-chip storage.  Maxwell and Pascal, by contrast, provide
                                 dedicated space to the shared memory of each SM, since the
                                 functionality of the L1 and texture caches have been merged.
                                 This increases the shared memory space available per SM
                                 as compared to Kepler: GP100 offers 64 KB shared memory per SM,
                                 and GP104 provides 96 KB per SM.
                              </p>
                              <div class="p">This presents several benefits to application developers:
                                 
                                 <ul class="ul">
                                    <li class="li">Algorithms with significant shared memory capacity
                                       requirements (e.g., radix sort) see an automatic 33% to
                                       100% boost in capacity per SM on top of the aggregate boost
                                       from higher SM count.
                                    </li>
                                    <li class="li">Applications no longer need to select a preference
                                       of the L1/shared split for optimal performance.
                                       For purposes of backward compatibility with Fermi and
                                       Kepler, applications may optionally continue to specify
                                       such a preference, but the preference will be ignored
                                       on Maxwell and Pascal.
                                    </li>
                                 </ul>
                              </div>
                              <div class="note note"><span class="notetitle">Note:</span> Thread-blocks remain limited to 48 KB of shared memory.
                                 For maximum flexibility, NVIDIA recommends that applications
                                 use at most 32 KB of shared memory in any one thread block.
                                 This would, for example, allow at least two thread blocks to
                                 fit per GP100 SM, or 3 thread blocks per GP104 SM.
                              </div>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="shared-memory-bandwidth"><a name="shared-memory-bandwidth" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#shared-memory-bandwidth" name="shared-memory-bandwidth" shape="rect">1.4.5.2.&nbsp;Shared Memory Bandwidth</a></h3>
                           <div class="body conbody">
                              <p class="p">Kepler provided an optional 8-byte shared memory banking
                                 mode, which had the potential to increase shared memory
                                 bandwidth per SM for shared memory accesses of 8 or 16 bytes.
                                 However, applications could only benefit from this when storing
                                 these larger elements in shared memory (i.e., integers and fp32
                                 values saw no benefit), and only when the developer explicitly
                                 opted in to the 8-byte bank mode via the API.
                              </p>
                              <p class="p">To simplify this, Pascal follows Maxwell in returning to
                                 fixed four-byte banks. This allows, all applications using
                                 shared memory to benefit from the higher bandwidth, without
                                 specifying any particular preference via the API.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="inter-gpu-communication"><a name="inter-gpu-communication" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#inter-gpu-communication" name="inter-gpu-communication" shape="rect">1.4.6.&nbsp;Inter-GPU Communication</a></h3>
                        <div class="topic concept nested3" xml:lang="en-US" id="nvlink"><a name="nvlink" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#nvlink" name="nvlink" shape="rect">1.4.6.1.&nbsp;NVLink Interconnect</a></h3>
                           <div class="body conbody">
                              <p class="p">NVLink is NVIDIA's new high-speed data interconnect. NVLink
                                 can be used to significantly increase performance for both
                                 GPU-to-GPU communication and for GPU access to system memory.
                                 GP100 supports up to four NVLink connections with each
                                 connection carrying up to 40 GB/s of bi-directional
                                 bandwidth.
                              </p>
                              <p class="p">NVLink operates transparently within the existing CUDA
                                 model. Transfers between NVLink-connected endpoints are
                                 automatically routed through NVLink, rather than PCIe. The
                                 <samp class="ph codeph">cudaDeviceEnablePeerAccess()</samp> API call remains
                                 necessary to enable direct transfers (over either PCIe or
                                 NVLink) between GPUs.  The
                                 <samp class="ph codeph">cudaDeviceCanAccessPeer()</samp> can be used to
                                 determine if peer access is possible between any pair of
                                 GPUs.
                              </p>
                           </div>
                        </div>
                        <div class="topic concept nested3" xml:lang="en-US" id="gpudirect"><a name="gpudirect" shape="rect">
                              <!-- --></a><h3 class="title topictitle2"><a href="#gpudirect" name="gpudirect" shape="rect">1.4.6.2.&nbsp;GPUDirect RDMA Bandwidth</a></h3>
                           <div class="body conbody">
                              <p class="p">GPUDirect RDMA allows third party devices such as network
                                 interface cards (NICs) to directly access GPU memory. This
                                 eliminates unnecessary copy buffers, lowers CPU overhead, and
                                 significantly decreases the latency of MPI send/receive
                                 messages from/to GPU memory. Pascal doubles the delivered RDMA
                                 bandwidth when reading data from the source GPU memory and
                                 writing to the target NIC memory over PCIe.
                              </p>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="preemption"><a name="preemption" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#preemption" name="preemption" shape="rect">1.4.7.&nbsp;Compute Preemption</a></h3>
                        <div class="body conbody">
                           <div class="p">Compute Preemption is a new feature specific to GP100. Compute
                              Preemption allows compute tasks running on the GPU to be interrupted
                              at instruction-level granularity. The execution context (registers,
                              shared memory, etc.) are swapped to GPU DRAM so that another 
                              application can be swapped in and run. Compute preemption offers two
                              key advantages for developers:
                              
                              <ul class="ul">
                                 <li class="li">Long-running kernels no longer need to be broken up into small
                                    timeslices to avoid an unresponsive graphical user interface or
                                    kernel timeouts when a GPU is used simultaneously for compute and
                                    graphics.
                                 </li>
                                 <li class="li">Interactive kernel debugging on a single-GPU system is now
                                    possible.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                     <div class="topic concept nested2" xml:lang="en-US" id="unified-memory"><a name="unified-memory" shape="rect">
                           <!-- --></a><h3 class="title topictitle2"><a href="#unified-memory" name="unified-memory" shape="rect">1.4.8.&nbsp;Unified Memory Improvements</a></h3>
                        <div class="body conbody">
                           <p class="p">Pascal offers new hardware capabilities to extend Unified Memory
                              (UM) support. An extended 49-bit virtual addressing space allows
                              Pascal GPUs to address the full 48-bit virtual address space of
                              modern CPUs as well as the memories of all GPUs in the system
                              through a single virtual address space, not limited by the physical
                              memory sizes of any one processor. Pascal GPUs also support memory
                              page faulting. Page faulting allows applications to access the same
                              managed memory allocations from both host and device without
                              explicit synchronization. It also removes the need for the CUDA
                              runtime to pre-synchronize <em class="ph i">all</em> managed memory allocations
                              before each kernel launch. Instead, when a kernel accesses a
                              non-resident memory page, it faults, and the page can be migrated
                              to the GPU memory on-demand, or mapped into the GPU address space
                              for access over PCIe/NVLink interfaces.
                           </p>
                           <p class="p">These features boost performance on Pascal for many typical UM
                              workloads. In cases where the UM heuristics prove suboptimal, 
                              further tuning is possible through a set of migration hints that can 
                              be added to the source code.
                           </p>
                           <p class="p">On supporting operating system platforms, any memory allocated with
                              the default OS allocator (for example, malloc or new) can be
                              accessed from both GPU and CPU code using the same pointer. In
                              fact, all system virtual memory can be accessed from the GPU. On
                              such systems, there is no need to explicitly allocate managed
                              memory using <samp class="ph codeph">cudaMallocManaged()</samp>.
                           </p>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic reference nested0" id="revision-history"><a name="revision-history" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#revision-history" name="revision-history" shape="rect">A.&nbsp;Revision History</a></h2>
                  <div class="body refbody">
                     <div class="section">
                        <h2 class="title sectiontitle">Version 1.0</h2>
                        <ul class="ul">
                           <li class="li">Initial Public Release</li>
                        </ul>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" id="notices-header"><a name="notices-header" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#notices-header" name="notices-header" shape="rect">Notices</a></h2>
                  <div class="topic reference nested1" id="notice"><a name="notice" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#notice" name="notice" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section">
                           <h3 class="title sectiontitle">Notice</h3>
                           <p class="p">ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND
                              SEPARATELY, "MATERIALS") ARE BEING PROVIDED "AS IS." NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE
                              WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS
                              FOR A PARTICULAR PURPOSE. 
                           </p>
                           <p class="p">Information furnished is believed to be accurate and reliable. However, NVIDIA Corporation assumes no responsibility for the
                              consequences of use of such information or for any infringement of patents or other rights of third parties that may result
                              from its use. No license is granted by implication of otherwise under any patent rights of NVIDIA Corporation. Specifications
                              mentioned in this publication are subject to change without notice. This publication supersedes and replaces all other information
                              previously supplied. NVIDIA Corporation products are not authorized as critical components in life support devices or systems
                              without express written approval of NVIDIA Corporation.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="trademarks"><a name="trademarks" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#trademarks" name="trademarks" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section">
                           <h3 class="title sectiontitle">Trademarks</h3>
                           <p class="p">NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation
                              in the U.S. and other countries.  Other company and product names may be trademarks of
                              the respective companies with which they are associated.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="copyright"><a name="copyright" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#copyright" name="copyright" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section">
                           <h3 class="title sectiontitle">Copyright</h3>
                           <p class="p">© 2012-<span class="ph">2017</span> NVIDIA Corporation. All rights reserved.
                           </p>
                           <p class="p">This product includes software developed by the Syncro Soft SRL (http://www.sync.ro/).</p>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="fn"><a name="fntarg_1" href="#fnsrc_1" shape="rect"><sup>1</sup></a>  Throughout this guide,
                  <dfn class="term">Fermi</dfn> refers to devices of compute capability 2.x,
                  <dfn class="term">Kepler</dfn> refers to devices of compute capability 3.x,
                  <dfn class="term">Maxwell</dfn> refers to devices of compute capability 5.x,
                  and <dfn class="term">Pascal</dfn> refers to device of compute capability 6.x.
               </div>
               <div class="fn"><a name="fntarg_2" href="#fnsrc_2" shape="rect"><sup>2</sup></a>  The
                  specific compute capabilities of GP100 and GP104 are 6.0 and 6.1, respectively.
                  The GP102 architecture is similar to GP104.
               </div>
               <div class="fn"><a name="fntarg_3" href="#fnsrc_3" shape="rect"><sup>3</sup></a>  The Tesla P100 has 56 SMs
                  enabled.
               </div>
               <div class="fn"><a name="fntarg_4" href="#fnsrc_4" shape="rect"><sup>4</sup></a>  As an exception, scattered writes to HBM2 see
                  some overhead from ECC but much less than the overhead with
                  similar access patterns on ECC-protected GDDR5 memory.
               </div>
               <div class="fn"><a name="fntarg_5" href="#fnsrc_5" shape="rect"><sup>5</sup></a>  Migratable, or
                  <dfn class="term">Unified Memory (UM)</dfn>, allocations are made with
                  <samp class="ph codeph">cudaMallocManaged()</samp> or, for systems with
                  Heterogeneous Memory Management (HMM) support,
                  <samp class="ph codeph">malloc()</samp>.
               </div>
               <div class="fn"><a name="fntarg_6" href="#fnsrc_6" shape="rect"><sup>6</sup></a>  Here [Op] would be one of <samp class="ph codeph">Add</samp>,
                  <samp class="ph codeph">CAS</samp>, etc.
               </div>
               
               <hr id="contents-end"></hr>
               
            </article>
         </div>
      </div>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/formatting/common.min.js"></script>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/scripts/google-analytics/google-analytics-write.js"></script>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/scripts/google-analytics/google-analytics-tracker.js"></script>
      <script type="text/javascript">var switchTo5x=true;</script><script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script><script type="text/javascript">stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false});</script></body>
</html>